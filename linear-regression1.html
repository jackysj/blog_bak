
<!DOCTYPE html>
<html lang="zh">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="./theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/font-awesome.min.css">


    <link href="//jacky-sj/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Jacky MSC Atom">


    <link rel="shortcut icon" href="/images/images.ico" type="image/x-icon">
    <link rel="icon" href="/images/images.ico" type="image/x-icon">

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="index, follow" />


<meta name="author" content="Jacky Gong" />
<meta name="description" content="线性回归 1" />
<meta name="keywords" content="data science, r, machine learning, regression">
<meta property="og:site_name" content="Jacky MSC"/>
<meta property="og:title" content="数据科学之机器学习2：线性回归1"/>
<meta property="og:description" content="线性回归 1"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./linear-regression1.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2014-03-30 19:02:00+08:00"/>
<meta property="article:modified_time" content="2017-03-29 20:30:00+08:00"/>
<meta property="article:author" content="./author/jacky-gong.html">
<meta property="article:section" content="Data Science"/>
<meta property="article:tag" content="data science"/>
<meta property="article:tag" content="r"/>
<meta property="article:tag" content="machine learning"/>
<meta property="article:tag" content="regression"/>
<meta property="og:image" content="/images/images.png">

  <title>Jacky MSC &ndash; 数据科学之机器学习2：线性回归1</title>

</head>
<body>
  <aside>
    <div>
      <a href=".">
        <img src="/images/images.png" alt="Jacky MSC" title="Jacky MSC">
      </a>
      <h1><a href=".">Jacky MSC</a></h1>

<p>Thoughts and Writings</p>
      <nav>
        <ul class="list">

          <li><a href="https://old.jacky-sj.com/" target="_blank">Old</a></li>
          <li><a href="mailto:jackycodemsc@gmail.com" target="_blank">Email</a></li>
          <li><a href="/about.html" target="_blank">About</a></li>
        </ul>
      </nav>

      <ul class="social">
        <li><a class="sc-weibo" href="http://weibo.com/jackycode" target="_blank"><i class="fa fa-weibo"></i></a></li>
        <li><a class="sc-github" href="https://github.com/jackysj" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a class="sc-rss" href="/feeds/all.atom.xml" target="_blank"><i class="fa fa-rss"></i></a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href=".">    Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/categories.html">Categories</a>
      <a href="/tags.html">Tags</a>

      <a href="//jacky-sj/feeds/all.atom.xml">    Atom
</a>

    </nav>

<article class="single">
  <header>
    <h1 id="linear-regression1">数据科学之机器学习2：线性回归1</h1>
    <p>
          Posted on 2014-03-30 in <a href="./category/data-science.html">Data Science</a>


    </p>
  </header>


  <div>
    <p><em>“文章原创，转载请注明出处”</em></p>
<hr>
<h4>一、回归分析</h4>
<hr>
<p>在统计分析中，最大的两支应该算是相关分析和回归分析。而回归分析应该是统计学的核心。回归分析，就是研究因变量 <span class="math">\(y\)</span> 与自变量 <span class="math">\(x\)</span> 之间的关系，存在条件数学期望：<span class="math">\(f(x)=E(y\|x)\)</span>。此时有：<span class="math">\(y=f(x)+\varepsilon\)</span>，一般假设<span class="math">\(\varepsilon \sim N(0,\sigma^2)\)</span>。</p>
<p>回归分析有很多变种：简单线性回归；多项式回归；Logistic回归；非参数回归；非线性回归等等。本篇就介绍最简单的线性回归，首先来看看一元线性回归。</p>
<hr>
<h4>二、一元线性回归</h4>
<hr>
<p>对于一元线性回归来说，<span class="math">\(f(x)\)</span> 就是线性的，则有：<span class="math">\(f(x)=E(y\|x)=\beta_0 + \beta_1 x\)</span>。通过已知的数据，可以估计出 <span class="math">\(\beta_0,\beta_1\)</span> 的估计值：<span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span>。那么就有 <span class="math">\(y\)</span> 的预测值：<span class="math">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>。</p>
<hr>
<h5>1. 如何计算 <span class="math">\(\beta_0,\beta_1\)</span> 的估计值 <span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span> 呢？</h5>
<hr>
<p>定义离差平方和：</p>
<div class="math">$$Q(\beta_0,\beta_1) = \sum_{i=1}^{n}(y_i-f(x_i))^2$$</div>
<p>显然，我们希望 <span class="math">\(f(x_i)\)</span> 的值与真实值 <span class="math">\(y_i\)</span> 越接近越好。那么就是需要离差平方和越小越好。则得到目标：</p>
<div class="math">$$ \min_{\beta_0,\beta_1}{\sum_{i=1}^{n}(y_i-f(x_i))^2} $$</div>
<p>如何寻找 <span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span> 使得上面方程达到最小呢？这个就需要对其对 <span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span> 求偏导，得到：</p>
<div class="math">$$\frac{\partial Q}{\partial \beta_0}=-2\sum_{i=1}^{n}{(y_i-\beta_0-\beta_1x_i)}$$</div>
<div class="math">$$\frac{\partial Q}{\partial \beta_1}=-2\sum_{i=1}^{n}{(y_i-\beta_0-\beta_1x_i)x_i}$$</div>
<p>令上述两式都等于0，计算得到：</p>
<div class="math">$$\hat{\beta}_0=\overline{y}-\hat{\beta}_1\overline{x}$$</div>
<div class="math">$$\hat{\beta}_1=\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(y_i-\overline{x})}}{\sum_{i=1}^{n}{(x-\overline{x})^2}}$$</div>
<p>这样就得到 <span class="math">\(\beta_0,\beta_1\)</span> 的估计值 <span class="math">\(\hat{\beta}_0,\hat{\beta}_1\)</span>。这个方法就叫做OLS，即普通最小二乘(ordinary least squares)。</p>
<hr>
<h5>2. R语言实现</h5>
<hr>
<p>在R语言中有自带的函数可以处理线性回归，那就是<code>lm</code>函数。这里使用自带的数据<code>cars</code>做演示：</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="o">&gt;</span> <span class="kn">attach</span><span class="p">(</span>cars<span class="p">)</span> <span class="c1"># 使用数据集cars，与with函数类似</span></span>
<span class="code-line"><span class="o">&gt;</span> lingre <span class="o">&lt;-</span> lm<span class="p">(</span>dist <span class="o">~</span> speed<span class="p">)</span></span>
<span class="code-line"><span class="o">&gt;</span> <span class="kp">summary</span><span class="p">(</span>lingre<span class="p">)</span></span>
<span class="code-line"></span>
<span class="code-line">Call<span class="o">:</span></span>
<span class="code-line">lm<span class="p">(</span>formula <span class="o">=</span> dist <span class="o">~</span> speed<span class="p">)</span></span>
<span class="code-line"></span>
<span class="code-line">Residuals<span class="o">:</span></span>
<span class="code-line">    Min      <span class="m">1</span>Q  Median      <span class="m">3</span>Q     Max</span>
<span class="code-line"><span class="m">-29.069</span>  <span class="m">-9.525</span>  <span class="m">-2.272</span>   <span class="m">9.215</span>  <span class="m">43.201</span></span>
<span class="code-line"></span>
<span class="code-line">Coefficients<span class="o">:</span></span>
<span class="code-line">            Estimate Std. Error t value Pr<span class="p">(</span><span class="o">&gt;|</span><span class="kp">t</span><span class="o">|</span><span class="p">)</span></span>
<span class="code-line"><span class="p">(</span>Intercept<span class="p">)</span> <span class="m">-17.5791</span>     <span class="m">6.7584</span>  <span class="m">-2.601</span>   <span class="m">0.0123</span> <span class="o">*</span></span>
<span class="code-line">speed         <span class="m">3.9324</span>     <span class="m">0.4155</span>   <span class="m">9.464</span> <span class="m">1.49e-12</span> <span class="o">***</span></span>
<span class="code-line"><span class="o">---</span></span>
<span class="code-line">Signif. codes<span class="o">:</span>  <span class="m">0</span> ‘<span class="o">***</span>’ <span class="m">0.001</span> ‘<span class="o">**</span>’ <span class="m">0.01</span> ‘<span class="o">*</span>’ <span class="m">0.05</span> ‘<span class="m">.</span>’ <span class="m">0.1</span> ‘ ’ <span class="m">1</span></span>
<span class="code-line"></span>
<span class="code-line">Residual standard error<span class="o">:</span> <span class="m">15.38</span> on <span class="m">48</span> degrees of freedom</span>
<span class="code-line">Multiple R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.6511</span><span class="p">,</span>    Adjusted R<span class="o">-</span>squared<span class="o">:</span>  <span class="m">0.6438</span></span>
<span class="code-line"><span class="bp">F</span><span class="o">-</span>statistic<span class="o">:</span> <span class="m">89.57</span> on <span class="m">1</span> and <span class="m">48</span> DF<span class="p">,</span>  p<span class="o">-</span>value<span class="o">:</span> <span class="m">1.49e-12</span></span>
<span class="code-line"></span>
<span class="code-line"><span class="o">&gt;</span> plot<span class="p">(</span>dist <span class="o">~</span> speed<span class="p">,</span> pch<span class="o">=</span><span class="m">4</span><span class="p">)</span> <span class="c1"># 画出散点图</span></span>
<span class="code-line"><span class="o">&gt;</span> abline<span class="p">(</span>lingre<span class="p">,</span> col<span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">)</span> <span class="c1"># 添加拟合直线</span></span>
<span class="code-line"><span class="o">&gt;</span> <span class="kn">detach</span><span class="p">(</span>cars<span class="p">)</span> <span class="c1"># 使用完记得释放</span></span>
</pre></div>


<p>从这里可以得到回归方程：<span class="math">\(\hat{dist} = -17.5791 + 3.9324 \times speed\)</span>。（对于其它的结果是什么意思，可以去查看线性回归的相关书籍）</p>
<p>另外，得到拟合直线的图像：</p>
<p><img alt="lingre_one" src="\images\article\a16\lingre_one.jpg"></p>
<hr>
<h4>三、多元线性回归</h4>
<hr>
<p>对于多元线性回归来说，其计算方式与一元线性回归类似，区别在于，多元的时候需要利用矩阵来处理。首先看一下回归模型：</p>
<div class="math">$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon $$</div>
<p>其中 <span class="math">\(p\)</span> 代表自变量的个数。</p>
<p>若取 <span class="math">\(x^T_0=[1, 1, \dots, 1]_{1 \times n}\)</span>，则可将上述模型改写成：<span class="math">\(y=X\beta+\varepsilon\)</span>。其中：</p>
<div class="math">$$y^T=[y_1,y_2,\dots,y_n], X=[x_0,x_1,\dots,x_p], \beta^T=[\beta_0,\beta_1,\dots,\beta_p], \varepsilon^T=[\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n]$$</div>
<p>其中 <span class="math">\(x^T_i=[x_{1i},x_{2i},\dots,x_{ni}]\)</span>。</p>
<p>这样我们就可以将离差平方和 <span class="math">\(\sum_{i=1}^{n}{(y_i-\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p)^2}\)</span> 写成矩阵形式：</p>
<div class="math">$$(y-X\beta)^T(y-X\beta)$$</div>
<p>求导可得：<span class="math">\(-2X^T(y-X\beta)\)</span> (这里用到矩阵求导的知识，一般介绍<strong>线性模型</strong>的书籍中会讲到；当然也可以直接对上面不是矩阵形式的离差平和求导)。令其等于0，可得：</p>
<div class="math">$$\hat{\beta} = (X^TX)^{-1}X^Ty$$</div>
<hr>
<h5>R语言实现</h5>
<hr>
<p>对于R语言的实现，依旧使用<code>lm</code>函数：</p>
<div class="highlight"><pre><span class="code-line"><span></span>lingre_mul <span class="o">&lt;-</span> lm<span class="p">(</span>y <span class="o">~</span> x1 <span class="o">+</span> x2<span class="p">,</span> data<span class="o">=</span>datasets<span class="p">)</span></span>
<span class="code-line"><span class="kp">summary</span><span class="p">(</span>lingre_mul<span class="p">)</span></span>
</pre></div>


<p>这里就不再用实际数据去演示了。</p>
<hr>
<h4>四、最后</h4>
<hr>
<p>至此，就把线性回归的基础内容介绍完了。但其实线性回归还存在很多其它的问题。比如说回归诊断（就是检查回归的效果），变量选择等等等等。感兴趣的话，可以找本讲线性回归的书看看，有很多！</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="./tag/data-science.html">data science</a>
      <a href="./tag/r.html">r</a>
      <a href="./tag/machine-learning.html">machine learning</a>
      <a href="./tag/regression.html">regression</a>
    </p>
  </div>


    <div class="addthis_relatedposts_inline">


<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'jacky-msc';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
        Please enable JavaScript to view comments.

</noscript>
</article>

    <footer>
<p>
  &copy; Jacky Gong 2014-2017 - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>    Powered by <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
         src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Jacky MSC ",
  "url" : ".",
  "image": "/images/images.png",
  "description": ""
}
</script>

</body>
</html>